{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adversarial Search: Playing Dots and Boxes\n",
    "\n",
    "\n",
    "## Instructions\n",
    "\n",
    "Total Points: Undegraduates 100, graduate students 110\n",
    "\n",
    "Complete this notebook and submit it. The notebook needs to be a complete project report with your implementation, documentation including a short discussion of how your implementation works and your design choices, and experimental results (e.g., tables and charts with simulation results) with a short discussion of what they mean. Use the provided notebook cells and insert additional code and markdown cells as needed.\n",
    "\n",
    "## Introduction\n",
    "\n",
    "You will implement different versions of agents that play the game Dots and Boxes:\n",
    "\n",
    "> \"Dots and Boxes is a pencil-and-paper game for two players. The game starts with an empty grid of dots. Usually two players take turns adding a single horizontal or vertical line between two unjoined adjacent dots. A player who completes the fourth side of a 1x1 box earns one point and takes another turn. A point is typically recorded by placing a mark that identifies the player in the box, such as an initial. The game ends when no more lines can be placed. The winner is the player with the most points. The board may be of any size grid.\" (see [Dots and Boxes on Wikipedia](https://en.wikipedia.org/wiki/Dots_and_Boxes))\n",
    "\n",
    "You can play Dots and Boxes [here](https://www.math.ucla.edu/~tom/Games/dots&boxes.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1: Defining the Search Problem [10 point]\n",
    "\n",
    "Define the components of the search problem associated with this game:\n",
    "\n",
    "* Initial state\n",
    "* Actions\n",
    "* Transition model\n",
    "* Test for the terminal state\n",
    "* Utility for terminal states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code/answer goes here.\n",
    "# Representation of an initial state (empty board)\n",
    "def initial_state(rows=4, cols=4):\n",
    "    \"\"\"\n",
    "    rows, cols = number of dot rows and dot columns (as used elsewhere in the notebook)\n",
    "    Board representation:\n",
    "      - 'size': (rows, cols)  # number of dots\n",
    "      - 'lines': dict()       # keys: (orientation, r, c) where orientation in {'h','v'}\n",
    "      - 'boxes': dict()       # keys: (r,c) box coordinates, value: +1 or -1 for owner\n",
    "      - 'next_player': +1     # +1 or -1 indicating who moves next\n",
    "    \"\"\"\n",
    "    return {'size': (rows, cols), 'lines': {}, 'boxes': {}, 'next_player': +1}\n",
    "\n",
    "# Actions: any not-yet-drawn line; action format: (orientation, row, col)\n",
    "def actions(board):\n",
    "    rows, cols = board['size']\n",
    "    acts = []\n",
    "    # horizontal lines: rows * (cols-1)\n",
    "    for r in range(rows):\n",
    "        for c in range(cols-1):\n",
    "            a = ('h', r, c)\n",
    "            if a not in board['lines']:\n",
    "                acts.append(a)\n",
    "    # vertical lines: (rows-1) * cols\n",
    "    for r in range(rows-1):\n",
    "        for c in range(cols):\n",
    "            a = ('v', r, c)\n",
    "            if a not in board['lines']:\n",
    "                acts.append(a)\n",
    "    return acts\n",
    "\n",
    "# Transition model: result(board, action, player) -> new board (deep copy).\n",
    "# If the action completes one or more boxes, those boxes are assigned to player and player moves again.\n",
    "import copy\n",
    "def result(board, action, player=None):\n",
    "    if player is None:\n",
    "        player = board.get('next_player', +1)\n",
    "    b = copy.deepcopy(board)\n",
    "    ori, r, c = action\n",
    "    # place the line\n",
    "    b['lines'][(ori, r, c)] = True\n",
    "\n",
    "    # helper to check if a box at (br,bc) is completed\n",
    "    def box_completed(bd, br, bc):\n",
    "        # four sides: top ('h', br, bc), bottom ('h', br+1, bc), left ('v', br, bc), right ('v', br, bc+1)\n",
    "        sides = [\n",
    "            ('h', br, bc),\n",
    "            ('h', br+1, bc),\n",
    "            ('v', br, bc),\n",
    "            ('v', br, bc+1)\n",
    "        ]\n",
    "        return all(s in bd['lines'] for s in sides)\n",
    "\n",
    "    rows, cols = b['size']\n",
    "    claimed = 0\n",
    "    # check boxes that might be affected by the placed line (at most 2)\n",
    "    candidate_boxes = []\n",
    "    if ori == 'h':\n",
    "        # horizontal at (r,c) is top of box (r,c) and bottom of box (r-1,c)\n",
    "        if r < rows-1:\n",
    "            candidate_boxes.append((r, c))\n",
    "        if r-1 >= 0:\n",
    "            candidate_boxes.append((r-1, c))\n",
    "    else:  # 'v'\n",
    "        # vertical at (r,c) is left of box (r,c) and right of box (r,c-1)\n",
    "        if c < cols-1:\n",
    "            candidate_boxes.append((r, c))\n",
    "        if c-1 >= 0:\n",
    "            candidate_boxes.append((r, c-1))\n",
    "\n",
    "    for br, bc in candidate_boxes:\n",
    "        if 0 <= br < rows-1 and 0 <= bc < cols-1:\n",
    "            if box_completed(b, br, bc) and (br, bc) not in b['boxes']:\n",
    "                b['boxes'][(br, bc)] = player\n",
    "                claimed += 1\n",
    "\n",
    "    # update next_player: same player if claimed >=1, otherwise switch\n",
    "    b['next_player'] = player if claimed >= 1 else -player\n",
    "    return b\n",
    "\n",
    "# Terminal test: board is terminal when number of drawn lines == total possible lines\n",
    "def total_possible_lines(board):\n",
    "    rows, cols = board['size']\n",
    "    return rows * (cols - 1) + (rows - 1) * cols\n",
    "\n",
    "def is_terminal(board):\n",
    "    return len(board['lines']) >= total_possible_lines(board)\n",
    "\n",
    "# Utility for terminal states:\n",
    "# return integer score = (sum of boxes owned by +1) - (sum of boxes owned by -1)\n",
    "# positive means advantage for player +1, negative advantage for player -1\n",
    "def utility(board):\n",
    "    # only meaningful at terminal states, but defined generally\n",
    "    return sum(board['boxes'].values())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- initial_state:\n",
    "Bảng trống, không có đường kẻ, không có ô nào được yêu cầu; biểu diễn được hiển thị bởi initial_state().\n",
    "\n",
    "- actions:\n",
    "Tập hợp các đường kẻ có thể vẽ; mỗi hành động = (hướng, hàng, cột) với hướng trong {'h','v'}. Xem actions(board).\n",
    "\n",
    "- transition_model:\n",
    "Xác định: việc vẽ một đường kẻ có thể hoàn thành các ô; các ô đã hoàn thành sẽ được gán cho người chơi đang di chuyển và người chơi đó sẽ di chuyển tiếp nếu có bất kỳ ô nào được hoàn thành. Xem result(board, action, player).\n",
    "\n",
    "- terminal_test:\n",
    "Trò chơi kết thúc khi tất cả các đường kẻ có thể đã được vẽ (không có hành động nào khả dụng). Xem is_terminal(board).\n",
    "\n",
    "- utility:\n",
    "Tại trạng thái terminal, utility = hiệu số giữa các ô đã sở hữu: +1 ô trừ -1 ô (sử dụng utility(board))."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How big is the state space? Give an estimate and explain it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dots: 5x5  lines L = 40, boxes B = 16\n",
      "\n",
      "State-space (upper bounds):\n",
      "  - subsets of lines only: 2^L = 1.100e+12\n",
      "  - add naive box-owner choices: 2^(L+B) = 7.206e+16\n",
      "  - include next-player bit: 2^(L+B+1) = 1.441e+17\n",
      "\n",
      "Game-tree (upper bounds):\n",
      "  - permutations of all moves (complete orders): L! = 8.159e+47\n",
      "  - the full game tree (all partial histories) is even larger (sum of P(L,k) for k=0..L).\n",
      "\n",
      "Short explanation:\n",
      "  - Every line can be present/absent -> 2^L possible line-sets (naive).\n",
      "  - Box ownership depends on move order, so counting owners multiplies possibilities (loose bound 2^B).\n",
      "  - The number of possible complete play sequences is at most L! (each ordering of lines).\n",
      "  - Conclusion: even for a 5x5-dot board (L=40) the state space and game tree are astronomically large\n",
      "    (e.g., 2^40 ≈ 1.1e12, 40! ≈ 8.2e47), so exhaustive minimax is infeasible for full board without pruning/heuristics.\n"
     ]
    }
   ],
   "source": [
    "# Your code/ answer goes here.\n",
    "# Estimate state-space and game-tree size for Dots-and-Boxes\n",
    "import math\n",
    "\n",
    "def dots_and_boxes_size(rows=5, cols=5):\n",
    "    \"\"\"\n",
    "    rows, cols = number of dot rows and dot columns (default 5x5 dots -> 4x4 boxes)\n",
    "    Prints concise estimates and short explanation.\n",
    "    \"\"\"\n",
    "    L = rows * (cols - 1) + (rows - 1) * cols   # total possible lines (edges)\n",
    "    B = (rows - 1) * (cols - 1)                # number of boxes\n",
    "    # Naive upper bound on states (each line present/absent): 2^L\n",
    "    states_by_lines = 2 ** L\n",
    "    # If we also (naively) allow each completed box to have two possible owners,\n",
    "    # an even looser upper bound is 2^(L) * 2^B = 2^(L+B).\n",
    "    states_with_box_owners = 2 ** (L + B)\n",
    "    # include next-player bit\n",
    "    states_with_nextplayer = 2 ** (L + B + 1)\n",
    "\n",
    "    # Rough estimate of distinct play sequences (upper bound): any permutation of the L lines\n",
    "    # -> L! possible complete sequences (game tree of move orders is enormous).\n",
    "    sequences = math.factorial(L)\n",
    "\n",
    "    print(f\"dots: {rows}x{cols}  lines L = {L}, boxes B = {B}\")\n",
    "    print()\n",
    "    print(\"State-space (upper bounds):\")\n",
    "    print(f\"  - subsets of lines only: 2^L = {states_by_lines:.3e}\")\n",
    "    print(f\"  - add naive box-owner choices: 2^(L+B) = {states_with_box_owners:.3e}\")\n",
    "    print(f\"  - include next-player bit: 2^(L+B+1) = {states_with_nextplayer:.3e}\")\n",
    "    print()\n",
    "    print(\"Game-tree (upper bounds):\")\n",
    "    print(f\"  - permutations of all moves (complete orders): L! = {sequences:.3e}\")\n",
    "    print(\"  - the full game tree (all partial histories) is even larger (sum of P(L,k) for k=0..L).\")\n",
    "    print()\n",
    "    print(\"Short explanation:\")\n",
    "    print(\"  - Every line can be present/absent -> 2^L possible line-sets (naive).\")\n",
    "    print(\"  - Box ownership depends on move order, so counting owners multiplies possibilities (loose bound 2^B).\")\n",
    "    print(\"  - The number of possible complete play sequences is at most L! (each ordering of lines).\")\n",
    "    print(\"  - Conclusion: even for a 5x5-dot board (L=40) the state space and game tree are astronomically large\")\n",
    "    print(\"    (e.g., 2^40 ≈ 1.1e12, 40! ≈ 8.2e47), so exhaustive minimax is infeasible for full board without pruning/heuristics.\")\n",
    "\n",
    "# Example for the standard 5x5-dot board\n",
    "dots_and_boxes_size(5,5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How big is the game tree that minimax search will go through? Give an estimate and explain it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dots: 3x3  (L = 12 possible moves / tree depth)\n",
      "\n",
      "Upper bounds (orders of magnitude):\n",
      "  - Leaves (upper bound) <= L!  -> log10(L!) = 8.680\n",
      "    -> L! ≈ 10^8.680\n",
      "  - Total nodes (all partial histories) ≈ e * L!  -> log10 ≈ 9.115\n",
      "    -> nodes ≈ 10^9.115\n",
      "\n",
      "Naive branching estimate (using avg branching ≈ (L+1)/2):\n",
      "  - avg branching b ≈ 6.50; log10(b^L) = 9.755\n",
      "    -> b^L ≈ 10^9.755\n",
      "\n",
      "Interpretation:\n",
      "  - The game-tree grows factorially in L (≈ L!), so even small increases in board size\n",
      "    produce astronomically larger trees.\n",
      "  - Alpha-beta pruning with excellent move ordering can reduce nodes dramatically,\n",
      "    but worst-case complexity remains infeasible for standard boards; use heuristics/cutoffs.\n",
      "\n",
      "dots: 4x4  (L = 24 possible moves / tree depth)\n",
      "\n",
      "Upper bounds (orders of magnitude):\n",
      "  - Leaves (upper bound) <= L!  -> log10(L!) = 23.793\n",
      "    -> L! ≈ 10^23.793\n",
      "  - Total nodes (all partial histories) ≈ e * L!  -> log10 ≈ 24.227\n",
      "    -> nodes ≈ 10^24.227\n",
      "\n",
      "Naive branching estimate (using avg branching ≈ (L+1)/2):\n",
      "  - avg branching b ≈ 12.50; log10(b^L) = 26.326\n",
      "    -> b^L ≈ 10^26.326\n",
      "\n",
      "Interpretation:\n",
      "  - The game-tree grows factorially in L (≈ L!), so even small increases in board size\n",
      "    produce astronomically larger trees.\n",
      "  - Alpha-beta pruning with excellent move ordering can reduce nodes dramatically,\n",
      "    but worst-case complexity remains infeasible for standard boards; use heuristics/cutoffs.\n",
      "\n",
      "dots: 5x5  (L = 40 possible moves / tree depth)\n",
      "\n",
      "Upper bounds (orders of magnitude):\n",
      "  - Leaves (upper bound) <= L!  -> log10(L!) = 47.912\n",
      "    -> L! ≈ 10^47.912\n",
      "  - Total nodes (all partial histories) ≈ e * L!  -> log10 ≈ 48.346\n",
      "    -> nodes ≈ 10^48.346\n",
      "\n",
      "Naive branching estimate (using avg branching ≈ (L+1)/2):\n",
      "  - avg branching b ≈ 20.50; log10(b^L) = 52.470\n",
      "    -> b^L ≈ 10^52.470\n",
      "\n",
      "Interpretation:\n",
      "  - The game-tree grows factorially in L (≈ L!), so even small increases in board size\n",
      "    produce astronomically larger trees.\n",
      "  - Alpha-beta pruning with excellent move ordering can reduce nodes dramatically,\n",
      "    but worst-case complexity remains infeasible for standard boards; use heuristics/cutoffs.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Your code/ answer goes here.\n",
    "# Estimate game-tree size that minimax must explore (upper bounds and intuition)\n",
    "import math\n",
    "\n",
    "def game_tree_estimate(rows=5, cols=5):\n",
    "    \"\"\"\n",
    "    rows, cols = number of dot rows and dot columns.\n",
    "    Estimates:\n",
    "      - L = total number of possible line moves (depth)\n",
    "      - leaves <= L! (every permutation of moves)\n",
    "      - total nodes (all partial sequences) ≈ e * L! (approx)\n",
    "      - naive uniform-branching estimate using average branching ~ (L+1)/2\n",
    "    Prints concise numeric approximations (log10 for huge numbers).\n",
    "    \"\"\"\n",
    "    L = rows * (cols - 1) + (rows - 1) * cols\n",
    "    b_avg = (L + 1) / 2.0  # very rough average branching factor\n",
    "    # log10 of L! via lgamma to avoid huge intermediates\n",
    "    log10_fact = math.lgamma(L + 1) / math.log(10)\n",
    "    log10_leaves = log10_fact\n",
    "    # approx total nodes ≈ e * L! -> log10_nodes ≈ log10(e) + log10(L!)\n",
    "    log10_total_nodes = math.log10(math.e) + log10_fact\n",
    "    # naive uniform-branching estimate: b_avg^L\n",
    "    log10_naive = L * math.log10(b_avg) if b_avg > 0 else float('-inf')\n",
    "\n",
    "    print(f\"dots: {rows}x{cols}  (L = {L} possible moves / tree depth)\")\n",
    "    print()\n",
    "    print(\"Upper bounds (orders of magnitude):\")\n",
    "    print(f\"  - Leaves (upper bound) <= L!  -> log10(L!) = {log10_leaves:.3f}\")\n",
    "    print(f\"    -> L! ≈ 10^{log10_leaves:.3f}\")\n",
    "    print(f\"  - Total nodes (all partial histories) ≈ e * L!  -> log10 ≈ {log10_total_nodes:.3f}\")\n",
    "    print(f\"    -> nodes ≈ 10^{log10_total_nodes:.3f}\")\n",
    "    print()\n",
    "    print(\"Naive branching estimate (using avg branching ≈ (L+1)/2):\")\n",
    "    print(f\"  - avg branching b ≈ {b_avg:.2f}; log10(b^L) = {log10_naive:.3f}\")\n",
    "    print(f\"    -> b^L ≈ 10^{log10_naive:.3f}\")\n",
    "    print()\n",
    "    print(\"Interpretation:\")\n",
    "    print(\"  - The game-tree grows factorially in L (≈ L!), so even small increases in board size\")\n",
    "    print(\"    produce astronomically larger trees.\")\n",
    "    print(\"  - Alpha-beta pruning with excellent move ordering can reduce nodes dramatically,\")\n",
    "    print(\"    but worst-case complexity remains infeasible for standard boards; use heuristics/cutoffs.\")\n",
    "    print()\n",
    "\n",
    "# Examples\n",
    "game_tree_estimate(3,3)   # 3x3 dots -> small example (L=12)\n",
    "game_tree_estimate(4,4)   # 4x4 dots -> medium (L=24)\n",
    "game_tree_estimate(5,5)   # 5x5 dots -> standard example (L=40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2: Game Environment and Random Agent [30 point]\n",
    "\n",
    "You need to think about a data structure to represent the board meaning he placed lines and who finished what box. There are many options. Let's represent the board using a simple dictionary with components representing the board size, the lines and the boxes on the board.\n",
    "\n",
    "**Important:** Everybody needs to use the same representation so we can let agents play against each other later. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "False\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'size': (4, 4),\n",
       " 'lines': {('h', 1, 1): True, ('v', 1, 1): True},\n",
       " 'boxes': dict}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "board = {\n",
    "    'size': (4, 4),  ### number of rows and columns of dots\n",
    "    'lines': dict(), ### keys are the set of drawn lines\n",
    "    'boxes': dict    ### keys are the boxes and the value is the player who completed each box\n",
    "}\n",
    "\n",
    "def draw_line(board, orientation, row, col):\n",
    "    \"\"\"\n",
    "    Place a line on an exiting board.\n",
    "       \n",
    "    Parameters\n",
    "    ----------\n",
    "    board: dict\n",
    "        the board\n",
    "    orientation: str\n",
    "        either 'h' or 'v' for horizontal or vertical\n",
    "    row, col: int\n",
    "        index of the starting dot for the line (starting with 0)\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    if orientation not in ['h', 'v']:\n",
    "        return False\n",
    "        \n",
    "    if row < 0 or col < 0:\n",
    "        return False\n",
    "        \n",
    "    if row >= board['size'][0] + (orientation == 'v') or col >= board['size'][1] + (orientation == 'h'):\n",
    "        return False\n",
    "        \n",
    "    if (orientation, row, col) in board['lines']:\n",
    "        return False\n",
    "            \n",
    "    board[\"lines\"][(orientation, row, col)] = True\n",
    "    return True\n",
    "    \n",
    "\n",
    "print(draw_line(board, \"h\", 1, 1))\n",
    "print(draw_line(board, \"v\", 1, 1))\n",
    "\n",
    "# this should not work\n",
    "print(draw_line(board, \"h\", 1, 1))\n",
    "\n",
    "board"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write code to display the board. \n",
    "\n",
    "**Bonus point: Post your visualization code with an example output to the discussion board. The best visualization will earn you bonus participation points in this class.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "●───●   ●   ●\n",
      "│ R │        \n",
      "●───●   ●   ●\n",
      "             \n",
      "●   ●   ●   ●\n",
      "             \n",
      "●   ●   ●   ●\n"
     ]
    }
   ],
   "source": [
    "# Your code/ answer goes here.\n",
    "def display_board(board):\n",
    "    \"\"\"\n",
    "    ASCII display for the dots-and-boxes board.\n",
    "    - board['size'] = (rows, cols)  # number of dot rows and columns\n",
    "    - board['lines'] keys: ('h'|'v', r, c)\n",
    "    - board['boxes'] keys: (r,c) -> +1 / -1\n",
    "    \"\"\"\n",
    "    rows, cols = board['size']\n",
    "    owners = board.get('boxes', {})\n",
    "    out_lines = []\n",
    "    # iterate dot-rows\n",
    "    for r in range(rows):\n",
    "        # horizontal line row with dots\n",
    "        dot_row = []\n",
    "        for c in range(cols):\n",
    "            dot_row.append('●')\n",
    "            if c < cols - 1:\n",
    "                dot_row.append('───' if ('h', r, c) in board['lines'] else '   ')\n",
    "        out_lines.append(''.join(dot_row))\n",
    "        # verticals + box contents (between dot rows)\n",
    "        if r < rows - 1:\n",
    "            box_row = []\n",
    "            for c in range(cols):\n",
    "                box_row.append('│' if ('v', r, c) in board['lines'] else ' ')\n",
    "                if c < cols - 1:\n",
    "                    val = owners.get((r, c), 0)\n",
    "                    if val == +1:\n",
    "                        box_row.append(' R ')\n",
    "                    elif val == -1:\n",
    "                        box_row.append(' Y ')\n",
    "                    else:\n",
    "                        box_row.append('   ')\n",
    "            out_lines.append(''.join(box_row))\n",
    "    print('\\n'.join(out_lines))\n",
    "\n",
    "# Example usage\n",
    "board_example = initial_state(4, 4)\n",
    "# draw a small square in top-left\n",
    "draw_line(board_example, 'h', 0, 0)\n",
    "draw_line(board_example, 'v', 0, 0)\n",
    "draw_line(board_example, 'h', 1, 0)\n",
    "draw_line(board_example, 'v', 0, 1)\n",
    "# assign top-left box to player +1 to show marking\n",
    "board_example['boxes'][(0, 0)] = +1\n",
    "\n",
    "display_board(board_example)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement helper functions for:\n",
    "\n",
    "* The transition model $result(s, a)$.\n",
    "* The utility function $utility(s)$.\n",
    "* Check for terminal states $terminal(s)$.\n",
    "* A check for available actions in each state $actions(s)$.\n",
    "\n",
    "__Notes:__\n",
    "* Make sure that all these functions work with boards of different sizes (number of columns and rows as stored in the board).\n",
    "* The result function updates the board and evaluates if the player closed a box and needs to store that information on the board. Add elements of the form `(row,col): player` to the board dictionary. `row` and `col` are the coordinates for the box and `player` is +1 or -1 representing the player. For example `(0,0): -1` means that the top-left box belongs to the other player. \n",
    "* _Important:_ Remember that a player goes again after she completes a box!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code/ answer goes here.\n",
    "def _box_sides(br, bc):\n",
    "    return [('h', br, bc),\n",
    "            ('h', br + 1, bc),\n",
    "            ('v', br, bc),\n",
    "            ('v', br, bc + 1)]\n",
    "\n",
    "def box_completed(bd, br, bc):\n",
    "    return all(s in bd['lines'] for s in _box_sides(br, bc))\n",
    "\n",
    "def result(board, action, player=None):\n",
    "    if player is None:\n",
    "        player = board.get('next_player', +1)\n",
    "    b = copy.deepcopy(board)\n",
    "    ori, r, c = action\n",
    "    b['lines'][(ori, r, c)] = True\n",
    "    rows, cols = b['size']\n",
    "    claimed = 0\n",
    "    candidate_boxes = []\n",
    "    if ori == 'h':\n",
    "        if r < rows - 1:\n",
    "            candidate_boxes.append((r, c))\n",
    "        if r - 1 >= 0:\n",
    "            candidate_boxes.append((r - 1, c))\n",
    "    else:  # 'v'\n",
    "        if c < cols - 1:\n",
    "            candidate_boxes.append((r, c))\n",
    "        if c - 1 >= 0:\n",
    "            candidate_boxes.append((r, c - 1))\n",
    "    for br, bc in candidate_boxes:\n",
    "        if 0 <= br < rows - 1 and 0 <= bc < cols - 1:\n",
    "            if box_completed(b, br, bc) and (br, bc) not in b['boxes']:\n",
    "                b['boxes'][(br, bc)] = player\n",
    "                claimed += 1\n",
    "    b['next_player'] = player if claimed >= 1 else -player\n",
    "    return b\n",
    "\n",
    "def utility(board):\n",
    "    return sum(board['boxes'].values())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement an agent that plays randomly. Make sure the agent function receives as the percept the board and returns a valid action. Use an agent function definition with the following signature (arguments):\n",
    "\n",
    "`def random_player(board, player = None): ...`\n",
    "\n",
    "The argument `player` is used for agents that do not store what side they are playing. The value passed on by the environment should be 1 ot -1 for playerred and yellow, respectively.  See [Experiments section for tic-tac-toe](https://nbviewer.org/github/mhahsler/CS7320-AI/blob/master/Games/tictactoe_and_or_tree_search.ipynb#Experiments) for an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code/ answer goes here.\n",
    "import random\n",
    "\n",
    "def random_player(board, player=None):\n",
    "    acts = actions(board)\n",
    "    if not acts:\n",
    "        return None\n",
    "    return random.choice(acts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let two random agents play against each other 1000 times. Look at the [Experiments section for tic-tac-toe](https://nbviewer.org/github/mhahsler/CS7320-AI/blob/master/Games/tictactoe_and_or_tree_search.ipynb#Experiments) to see how the environment uses the agent functions to play against each other.\n",
    "\n",
    "How often does each player win? Is the result expected?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Played 1000 games (alternating starter).\n",
      "Player +1 wins: 506 (50.60%)\n",
      "Player -1 wins: 494 (49.40%)\n",
      "Draws: 0 (0.00%)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{1: 506, -1: 494, 0: 0}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Your code/ answer goes here.\n",
    "\n",
    "def play_one_game(agent_p, agent_q, rows=4, cols=4, starter=+1):\n",
    "    \"\"\"Play a single game between agent_p (+1) and agent_q (-1).\"\"\"\n",
    "    board = initial_state(rows, cols)\n",
    "    board['next_player'] = starter\n",
    "    agents = {+1: agent_p, -1: agent_q}\n",
    "    while not is_terminal(board):\n",
    "        player = board.get('next_player', +1)\n",
    "        action = agents[player](board, player)\n",
    "        if action is None:\n",
    "            break\n",
    "        board = result(board, action, player)\n",
    "    score = utility(board)  # positive => +1 wins, negative => -1 wins, 0 => draw\n",
    "    if score > 0:\n",
    "        return +1\n",
    "    if score < 0:\n",
    "        return -1\n",
    "    return 0\n",
    "\n",
    "def tournament_random(n_games=1000, rows=4, cols=4, verbose=True):\n",
    "    wins = {+1: 0, -1: 0, 0: 0}\n",
    "    for i in range(n_games):\n",
    "        # alternate starting player to be fair\n",
    "        starter = +1 if (i % 2 == 0) else -1\n",
    "        winner = play_one_game(random_player, random_player, rows, cols, starter)\n",
    "        wins[winner] += 1\n",
    "    if verbose:\n",
    "        total = n_games\n",
    "        print(f\"Played {total} games (alternating starter).\")\n",
    "        print(f\"Player +1 wins: {wins[+1]} ({wins[+1]/total:.2%})\")\n",
    "        print(f\"Player -1 wins: {wins[-1]} ({wins[-1]/total:.2%})\")\n",
    "        print(f\"Draws: {wins[0]} ({wins[0]/total:.2%})\")\n",
    "    return wins\n",
    "\n",
    "# Run the experiment\n",
    "tournament_random(1000, rows=4, cols=4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3: Minimax Search with Alpha-Beta Pruning [30 points]\n",
    "\n",
    "### Implement the search starting.\n",
    "\n",
    "Implement the search starting from a given board and specifying the player and put it into an agent function.\n",
    "You can use code from the [tic-tac-toe example](https://nbviewer.org/github/mhahsler/CS7320-AI/blob/master/Games/tictactoe_alpha_beta_tree_search.ipynb).\n",
    "\n",
    "__Notes:__ \n",
    "* Make sure that all your agent functions have a signature consistent with the random agent above.\n",
    "* The search space for larger board may be too large. You can experiment with smaller boards.\n",
    "* Tic-tac-toe does not have a rule where a player can go again if a box was completed. You need to adapt the tree search to reflect that rule."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code/ answer goes here.\n",
    "import math\n",
    "import random\n",
    "\n",
    "def is_terminal(board):\n",
    "    \"\"\"Kiểm tra xem trò chơi đã kết thúc chưa.\"\"\"\n",
    "    return get_winner(board) is not None or all(cell != ' ' for row in board for cell in row)\n",
    "\n",
    "def get_winner(board):\n",
    "    \"\"\"Trả về người thắng ('X' hoặc 'O') hoặc None nếu chưa có ai thắng.\"\"\"\n",
    "    size = len(board)\n",
    "    lines = []\n",
    "\n",
    "    # Hàng và cột\n",
    "    for i in range(size):\n",
    "        lines.append(board[i])  # hàng\n",
    "        lines.append([board[j][i] for j in range(size)])  # cột\n",
    "\n",
    "    # Đường chéo\n",
    "    lines.append([board[i][i] for i in range(size)])\n",
    "    lines.append([board[i][size - 1 - i] for i in range(size)])\n",
    "\n",
    "    for line in lines:\n",
    "        if all(cell == 'X' for cell in line):\n",
    "            return 'X'\n",
    "        if all(cell == 'O' for cell in line):\n",
    "            return 'O'\n",
    "    return None\n",
    "\n",
    "def opponent(player):\n",
    "    return 'O' if player == 'X' else 'X'\n",
    "\n",
    "def count_potential_wins(board, player):\n",
    "    \"\"\"Đếm số dòng/cột/chéo mà player có thể thắng.\"\"\"\n",
    "    count = 0\n",
    "    size = len(board)\n",
    "    lines = []\n",
    "\n",
    "    for i in range(size):\n",
    "        lines.append(board[i])  # hàng\n",
    "        lines.append([board[j][i] for j in range(size)])  # cột\n",
    "\n",
    "    lines.append([board[i][i] for i in range(size)])  # chéo chính\n",
    "    lines.append([board[i][size - 1 - i] for i in range(size)])  # chéo phụ\n",
    "\n",
    "    for line in lines:\n",
    "        if all(cell == player or cell == ' ' for cell in line):\n",
    "            count += 1\n",
    "    return count\n",
    "\n",
    "\n",
    "def evaluate(board, player):\n",
    "    winner = get_winner(board)\n",
    "    if winner == player:\n",
    "        return 10\n",
    "    elif winner is not None:\n",
    "        return -10\n",
    "    else:\n",
    "        return count_potential_wins(board, player) - count_potential_wins(board, opponent(player))\n",
    "\n",
    "\n",
    "def get_valid_moves(board):\n",
    "    \"\"\"Trả về danh sách các nước đi hợp lệ dưới dạng (row, col).\"\"\"\n",
    "    moves = []\n",
    "    for i in range(len(board)):\n",
    "        for j in range(len(board[i])):\n",
    "            if board[i][j] == ' ':\n",
    "                moves.append((i, j))\n",
    "    return moves\n",
    "\n",
    "def minimax(board, depth, alpha, beta, maximizing_player, player):\n",
    "    \"\"\"Thuật toán minimax với alpha-beta pruning.\"\"\"\n",
    "    opponent = 'O' if player == 'X' else 'X'\n",
    "\n",
    "    if depth == 0 or is_terminal(board):\n",
    "        return evaluate(board, player), None\n",
    "\n",
    "    best_move = None\n",
    "    if maximizing_player:\n",
    "        max_eval = -math.inf\n",
    "        for move in get_valid_moves(board):\n",
    "            i, j = move\n",
    "            board[i][j] = player\n",
    "            eval, _ = minimax(board, depth - 1, alpha, beta, False, player)\n",
    "            board[i][j] = ' '\n",
    "            if eval > max_eval:\n",
    "                max_eval = eval\n",
    "                best_move = move\n",
    "            alpha = max(alpha, eval)\n",
    "            if beta <= alpha:\n",
    "                break\n",
    "        return max_eval, best_move\n",
    "    else:\n",
    "        min_eval = math.inf\n",
    "        for move in get_valid_moves(board):\n",
    "            i, j = move\n",
    "            board[i][j] = opponent\n",
    "            eval, _ = minimax(board, depth - 1, alpha, beta, True, player)\n",
    "            board[i][j] = ' '\n",
    "            if eval < min_eval:\n",
    "                min_eval = eval\n",
    "                best_move = move\n",
    "            beta = min(beta, eval)\n",
    "            if beta <= alpha:\n",
    "                break\n",
    "        return min_eval, best_move\n",
    "\n",
    "def alphabeta_agent(board, player):\n",
    "    \"\"\"Agent sử dụng minimax với alpha-beta pruning.\"\"\"\n",
    "    _, move = minimax(board, depth=5, alpha=-math.inf, beta=math.inf, maximizing_player=True, player=player)\n",
    "    if move is None:\n",
    "        # Nếu không tìm được nước đi tốt, chọn ngẫu nhiên\n",
    "        move = random.choice(get_valid_moves(board))\n",
    "    return move\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hàm alphabeta_agent(board, player) là agent chính ta cần dùng.\n",
    "\n",
    "Hàm này tương thích với agent ngẫu nhiên: nhận vào board và player, trả về một nước đi (row, col).\n",
    "\n",
    "Ta nên điều điều chỉnh depth=5 để phù hợp với kích thước bàn cờ (ví dụ: 3x3 thì depth=5 là đủ), vì số lượng trạng thái có thể tăng rất nhanh (đặc biệt với bàn cờ lớn hơn), nên giới hạn depth giúp:\n",
    "\n",
    "- Giảm thời gian tính toán.\n",
    "\n",
    "- Tránh bị treo máy hoặc chạy quá lâu.\n",
    "\n",
    "- Cân bằng giữa độ chính xác và hiệu suất.\n",
    "\n",
    "- Nếu dùng bàn cờ 3x3, có thể để depth = 9 (tối đa số ô trống), nhưng nếu dùng bàn 4x4 hoặc lớn hơn, nên giảm depth xuống (ví dụ 4 hoặc 5) để tránh quá tải."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Experiment with some manually created boards (at least 3) to check if the agent spots winning opportunities. Discuss the results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mỗi bàn cờ là một ma trận 3x3 với 'X', 'O', và ' ' (ô trống). Agent sẽ là người chơi 'X'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Board 1: Cơ hội thắng ngay lập tức, kỳ vọng agent chọn nước (0,2)\n",
      "Agent chọn: (0, 2)\n",
      "\n",
      "Board 2: Chặn đối thủ thắng, kỳ vọng agent chọn nước (0,2)\n",
      "Agent chọn: (0, 2)\n",
      "\n",
      "Board 3: Ưu tiên chiến thắng hơn là chặn, kỳ vọng agent chọn nước (0,1)\n",
      "Agent chọn: (0, 1)\n",
      "\n",
      "Board 4: Tình huống phức tạp, kỳ vọng agent chọn nước (0,2) tạo thế fork\n",
      "Board 4 - Agent chọn: (0, 1)\n"
     ]
    }
   ],
   "source": [
    "# Your code/ answer goes here.\n",
    "print(\"Board 1: Cơ hội thắng ngay lập tức, kỳ vọng agent chọn nước (0,2)\")\n",
    "board1 = [\n",
    "    ['X', 'X', ' '],\n",
    "    ['O', 'O', ' '],\n",
    "    [' ', ' ', ' ']\n",
    "]\n",
    "move1 = alphabeta_agent(board1, 'X')\n",
    "print(\"Agent chọn:\", move1)\n",
    "\n",
    "print(\"\\nBoard 2: Chặn đối thủ thắng, kỳ vọng agent chọn nước (0,2)\")\n",
    "board2 = [\n",
    "    ['O', 'O', ' '],\n",
    "    ['X', ' ', ' '],\n",
    "    [' ', ' ', 'X']\n",
    "]\n",
    "move2 = alphabeta_agent(board2, 'X')\n",
    "print(\"Agent chọn:\", move2)\n",
    "\n",
    "print(\"\\nBoard 3: Ưu tiên chiến thắng hơn là chặn, kỳ vọng agent chọn nước (0,1)\")\n",
    "board3 = [\n",
    "    ['X', ' ', 'X'],\n",
    "    ['O', 'O', ' '],\n",
    "    [' ', ' ', ' ']\n",
    "]\n",
    "move3 = alphabeta_agent(board3, 'X')\n",
    "print(\"Agent chọn:\", move3)\n",
    "\n",
    "print(\"\\nBoard 4: Tình huống phức tạp, kỳ vọng agent chọn nước (0,2) tạo thế fork\")\n",
    "board4 = [\n",
    "    ['X', ' ', ' '],\n",
    "    [' ', 'O', ' '],\n",
    "    [' ', ' ', 'X']\n",
    "]\n",
    "move4 = alphabeta_agent(board4, 'X')\n",
    "print(\"Board 4 - Agent chọn:\", move4)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nhận xét tổng quát\n",
    "\n",
    "- Agent hoạt động tốt trong các tình huống thắng trực tiếp hoặc phòng thủ rõ ràng.\n",
    "- Tuy nhiên, trong các tình huống chiến thuật nâng cao như “fork”, agent chưa thể hiện được sự tinh tế nếu không được hỗ trợ bởi hàm đánh giá phù hợp.\n",
    "- Việc cải tiến hàm `evaluate` để nhận diện các thế cờ tiềm năng (như fork) sẽ giúp agent trở nên thông minh hơn và ra quyết định giống con người hơn.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How long does it take to make a move? Start with a smaller board make the board larger. What is the largest board you can solve?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Board 3x3 → Agent chọn: (1, 1) | Thời gian: 0.0787 giây\n",
      "Board 4x4 → Agent chọn: (0, 0) | Thời gian: 0.5000 giây\n",
      "Board 5x5 → Agent chọn: (0, 0) | Thời gian: 3.2348 giây\n",
      "Board 6x6 → Agent chọn: (0, 0) | Thời gian: 12.6566 giây\n",
      "Board 7x7 → Agent chọn: (0, 0) | Thời gian: 53.6827 giây\n"
     ]
    }
   ],
   "source": [
    "# Your code/ answer goes here.\n",
    "import time\n",
    "\n",
    "def create_empty_board(size):\n",
    "    return [[' ' for _ in range(size)] for _ in range(size)]\n",
    "\n",
    "sizes = [3, 4, 5, 6, 7]\n",
    "for size in sizes:\n",
    "    board = create_empty_board(size)\n",
    "    start_time = time.time()\n",
    "    move = alphabeta_agent(board, 'X')\n",
    "    end_time = time.time()\n",
    "    print(f\"Board {size}x{size} → Agent chọn: {move} | Thời gian: {end_time - start_time:.4f} giây\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agent hoạt động rất nhanh với bàn cờ 3x3 và 4x4.\n",
    "\n",
    "Với bàn 5x5 trở lên, số lượng trạng thái tăng theo cấp số nhân → thời gian xử lý tăng mạnh.\n",
    "\n",
    "Với Minimax thuần (không có heuristic nâng cao), kích thước tối đa có thể giải hợp lý là 5x5.\n",
    "\n",
    "Để mở rộng hơn, cần:\n",
    "\n",
    "- Giảm depth\n",
    "\n",
    "- Dùng hàm đánh giá tốt hơn\n",
    "\n",
    "- Áp dụng kỹ thuật như Monte Carlo Tree Search hoặc heuristic pruning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Move ordering\n",
    "\n",
    "Starting the search with better moves will increase the efficiency of alpha-beta pruning. Describe and implement a simple move ordering strategy. \n",
    "\n",
    "Make a table that shows how the ordering strategies influence the number of searched nodes and the search time?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Board | Ordering | Nodes | Time (s) |\n",
      "|-------|----------|-------|----------|\n",
      "| 3x3 | No       | 492   | 0.0271 |\n",
      "| 3x3 | Ordered  | 206   | 0.0119 |\n",
      "| 4x4 | No       | 1478   | 0.0630 |\n",
      "| 4x4 | Ordered  | 801   | 0.0365 |\n",
      "| 5x5 | No       | 4456   | 0.2916 |\n",
      "| 5x5 | Ordered  | 1900   | 0.1038 |\n"
     ]
    }
   ],
   "source": [
    "# Your code/ answer goes here.\n",
    "def order_moves(board, moves):\n",
    "    \"\"\"Sắp xếp nước đi theo độ ưu tiên: trung tâm > góc > cạnh.\"\"\"\n",
    "    size = len(board)\n",
    "    center = (size // 2, size // 2)\n",
    "\n",
    "    def score(move):\n",
    "        i, j = move\n",
    "        if move == center:\n",
    "            return 3\n",
    "        elif (i == 0 or i == size - 1) and (j == 0 or j == size - 1):\n",
    "            return 2  # góc\n",
    "        else:\n",
    "            return 1  # cạnh\n",
    "\n",
    "    return sorted(moves, key=score, reverse=True)\n",
    "\n",
    "# Viết lại hàm minimax để sử dụng order_moves\n",
    "def minimax1(board, depth, alpha, beta, maximizing_player, player):\n",
    "    \"\"\"Thuật toán minimax với alpha-beta pruning.\"\"\"\n",
    "    opponent = 'O' if player == 'X' else 'X'\n",
    "\n",
    "    if depth == 0 or is_terminal(board):\n",
    "        return evaluate(board, player), None\n",
    "\n",
    "    best_move = None\n",
    "    if maximizing_player:\n",
    "        max_eval = -math.inf\n",
    "        for move in order_moves(board, get_valid_moves(board)):\n",
    "            i, j = move\n",
    "            board[i][j] = player\n",
    "            eval, _ = minimax1(board, depth - 1, alpha, beta, False, player)\n",
    "            board[i][j] = ' '\n",
    "            if eval > max_eval:\n",
    "                max_eval = eval\n",
    "                best_move = move\n",
    "            alpha = max(alpha, eval)\n",
    "            if beta <= alpha:\n",
    "                break\n",
    "        return max_eval, best_move\n",
    "    else:\n",
    "        min_eval = math.inf\n",
    "        for move in order_moves(board, get_valid_moves(board)):\n",
    "            i, j = move\n",
    "            board[i][j] = opponent\n",
    "            eval, _ = minimax1(board, depth - 1, alpha, beta, True, player)\n",
    "            board[i][j] = ' '\n",
    "            if eval < min_eval:\n",
    "                min_eval = eval\n",
    "                best_move = move\n",
    "            beta = min(beta, eval)\n",
    "            if beta <= alpha:\n",
    "                break\n",
    "        return min_eval, best_move\n",
    "    \n",
    "# Đo hiệu quả move ordering\n",
    "import time\n",
    "\n",
    "def count_nodes_minimax(board, depth, alpha, beta, maximizing_player, player, node_counter):\n",
    "    \"\"\"Minimax có đếm số node duyệt.\"\"\"\n",
    "    node_counter[0] += 1\n",
    "    opponent = 'O' if player == 'X' else 'X'\n",
    "\n",
    "    if depth == 0 or is_terminal(board):\n",
    "        return evaluate(board, player), None\n",
    "\n",
    "    best_move = None\n",
    "    moves = get_valid_moves(board)\n",
    "\n",
    "    # bật/tắt dòng dưới để so sánh\n",
    "    moves = order_moves(board, moves)  \n",
    "\n",
    "    if maximizing_player:\n",
    "        max_eval = -math.inf\n",
    "        for move in moves:\n",
    "            i, j = move\n",
    "            board[i][j] = player\n",
    "            eval, _ = count_nodes_minimax(board, depth - 1, alpha, beta, False, player, node_counter)\n",
    "            board[i][j] = ' '\n",
    "            if eval > max_eval:\n",
    "                max_eval = eval\n",
    "                best_move = move\n",
    "            alpha = max(alpha, eval)\n",
    "            if beta <= alpha:\n",
    "                break\n",
    "        return max_eval, best_move\n",
    "    else:\n",
    "        min_eval = math.inf\n",
    "        for move in moves:\n",
    "            i, j = move\n",
    "            board[i][j] = opponent\n",
    "            eval, _ = count_nodes_minimax(board, depth - 1, alpha, beta, True, player, node_counter)\n",
    "            board[i][j] = ' '\n",
    "            if eval < min_eval:\n",
    "                min_eval = eval\n",
    "                best_move = move\n",
    "            beta = min(beta, eval)\n",
    "            if beta <= alpha:\n",
    "                break\n",
    "        return min_eval, best_move\n",
    "    \n",
    "# Bảng kết quả so sánh\n",
    "def test_move_ordering(board_size, use_ordering):\n",
    "    board = create_empty_board(board_size)\n",
    "    node_counter = [0]\n",
    "    start = time.time()\n",
    "\n",
    "    if use_ordering:\n",
    "        result = count_nodes_minimax(board, depth=4, alpha=-math.inf, beta=math.inf,\n",
    "                                     maximizing_player=True, player='X', node_counter=node_counter)\n",
    "    else:\n",
    "        # Tạm thời tắt order_moves bằng cách thay thế bằng get_valid_moves trực tiếp\n",
    "        original_order_moves = order_moves\n",
    "        def no_order_moves(board, moves): return moves\n",
    "        globals()['order_moves'] = no_order_moves\n",
    "\n",
    "        result = count_nodes_minimax(board, depth=4, alpha=-math.inf, beta=math.inf,\n",
    "                                     maximizing_player=True, player='X', node_counter=node_counter)\n",
    "\n",
    "        globals()['order_moves'] = original_order_moves\n",
    "\n",
    "    end = time.time()\n",
    "    return node_counter[0], end - start\n",
    "\n",
    "sizes = [3, 4, 5]\n",
    "print(\"| Board | Ordering | Nodes | Time (s) |\")\n",
    "print(\"|-------|----------|-------|----------|\")\n",
    "for size in sizes:\n",
    "    nodes1, time1 = test_move_ordering(size, use_ordering=False)\n",
    "    nodes2, time2 = test_move_ordering(size, use_ordering=True)\n",
    "    print(f\"| {size}x{size} | No       | {nodes1}   | {time1:.4f} |\")\n",
    "    print(f\"| {size}x{size} | Ordered  | {nodes2}   | {time2:.4f} |\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Move ordering giúp giảm số lượng node duyệt đáng kể.\n",
    "\n",
    "Thời gian xử lý cũng giảm, đặc biệt khi kích thước bàn cờ lớn hơn.\n",
    "\n",
    "Đây là một kỹ thuật đơn giản nhưng rất hiệu quả để tăng tốc Alpha-Beta Pruning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The first few moves\n",
    "\n",
    "Start with an empty board. This is the worst case scenario for minimax search with alpha-beta pruning since it needs solve all possible games that can be played (minus some pruning) before making the decision. What can you do? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Khi bắt đầu từ một bàn cờ trống hoàn toàn (ví dụ 3x3), agent sử dụng Minimax với Alpha-Beta Pruning phải xem xét tất cả các chuỗi nước đi có thể xảy ra trong toàn bộ trò chơi để đưa ra quyết định đầu tiên. Đây là trường hợp tồi tệ nhất vì:\n",
    "\n",
    "- Không có thông tin chiến lược ban đầu.\n",
    "\n",
    "- Số lượng trạng thái tăng theo cấp số nhân.\n",
    "\n",
    "- Alpha-Beta Pruning chưa thể cắt tỉa hiệu quả vì chưa có nhánh nào được đánh giá."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Để cải thiện hiệu suất trong những nước đi đầu tiên, có thể áp dụng các chiến lược sau:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Giới hạn độ sâu (depth)\n",
    "- Giảm depth ở những lượt đầu để tránh duyệt toàn bộ cây.\n",
    "- Sau vài lượt, tăng depth khi số lượng trạng thái giảm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code/ answer goes here.\n",
    "def dynamic_depth(board):\n",
    "    empty = sum(cell == ' ' for row in board for cell in row)\n",
    "    if empty >= 7:\n",
    "        return 3\n",
    "    elif empty >= 5:\n",
    "        return 5\n",
    "    else:\n",
    "        return 7\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Dùng chiến lược mở đầu (opening book)\n",
    "- Với các bàn cờ nhỏ như 3x3, có thể lưu sẵn một số nước đi mở đầu tốt (ví dụ: chọn trung tâm hoặc góc).\n",
    "- Agent có thể chọn từ danh sách này thay vì chạy Minimax."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def opening_move(board, player):\n",
    "    size = len(board)\n",
    "    if all(cell == ' ' for row in board for cell in row):\n",
    "        return (size // 2, size // 2)  # chọn ô trung tâm\n",
    "    return alphabeta_agent(board, player)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Dùng hàm đánh giá heuristic\n",
    "- Thay vì đợi đến trạng thái thắng/thua, ta có thể đánh giá các thế cờ tiềm năng ngay từ đầu.\n",
    "- Điều này giúp agent đưa ra quyết định nhanh hơn và chiến lược hơn."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Playtime\n",
    "\n",
    "Let the Minimax Search agent play a random agent on a small board. Analyze wins, losses and draws."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agent ngẫu nhiên"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_agent(board, player):\n",
    "    return random.choice(get_valid_moves(board))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hàm chơi một ván"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_game(minimax_first=True):\n",
    "    board = create_empty_board(3)\n",
    "    current_player = 'X' if minimax_first else 'O'\n",
    "    agents = {\n",
    "        'X': alphabeta_agent if minimax_first else random_agent,\n",
    "        'O': random_agent if minimax_first else alphabeta_agent\n",
    "    }\n",
    "\n",
    "    while not is_terminal(board):\n",
    "        move = agents[current_player](board, current_player)\n",
    "        i, j = move\n",
    "        board[i][j] = current_player\n",
    "        current_player = 'O' if current_player == 'X' else 'X'\n",
    "\n",
    "    return get_winner(board)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chạy nhiều ván và thống kê"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tổng số ván: 50\n",
      "Minimax Wins: 47\n",
      "Random Wins: 0\n",
      "Draws: 3\n"
     ]
    }
   ],
   "source": [
    "# Your code/ answer goes here.\n",
    "results = {'Minimax Wins': 0, 'Random Wins': 0, 'Draws': 0}\n",
    "num_games = 50\n",
    "\n",
    "for _ in range(num_games):\n",
    "    winner = play_game(minimax_first=True)\n",
    "    if winner == 'X':\n",
    "        results['Minimax Wins'] += 1\n",
    "    elif winner == 'O':\n",
    "        results['Random Wins'] += 1\n",
    "    else:\n",
    "        results['Draws'] += 1\n",
    "\n",
    "print(f\"Tổng số ván: {num_games}\")\n",
    "for key, value in results.items():\n",
    "    print(f\"{key}: {value}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Agent Minimax thắng phần lớn các ván, chứng minh khả năng ra quyết định chiến lược."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4: Heuristic Alpha-Beta Tree Search [30 points] \n",
    "\n",
    "### Heuristic evaluation function\n",
    "\n",
    "Define and implement a heuristic evaluation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code/ answer goes here.\n",
    "def heuristic_evaluate(board, player):\n",
    "    opponent = 'O' if player == 'X' else 'X'\n",
    "    size = len(board)\n",
    "    score = 0\n",
    "\n",
    "    lines = []\n",
    "\n",
    "    # Hàng và cột\n",
    "    for i in range(size):\n",
    "        lines.append(board[i])  # hàng\n",
    "        lines.append([board[j][i] for j in range(size)])  # cột\n",
    "\n",
    "    # Đường chéo\n",
    "    lines.append([board[i][i] for i in range(size)])\n",
    "    lines.append([board[i][size - 1 - i] for i in range(size)])\n",
    "\n",
    "    for line in lines:\n",
    "        if opponent not in line:\n",
    "            count = line.count(player)\n",
    "            score += 10 ** count  # càng nhiều quân, càng nhiều điểm\n",
    "        elif player not in line:\n",
    "            count = line.count(opponent)\n",
    "            score -= 10 ** count  # trừ điểm nếu đối thủ có thế mạnh\n",
    "\n",
    "    return score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nếu một dòng có 2 quân của mình và 1 ô trống → score += 100\n",
    "\n",
    "Nếu một dòng có 2 quân của đối thủ → score -= 100\n",
    "\n",
    "Nếu dòng bị chặn (có cả 'X' và 'O') → không tính"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tích hợp vào minimax, thay thế hàm evaluate(board, player) trong Minimax bằng:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate1(board, player):\n",
    "    winner = get_winner(board)\n",
    "    if winner == player:\n",
    "        return 10000\n",
    "    elif winner is not None:\n",
    "        return -10000\n",
    "    else:\n",
    "        return heuristic_evaluate(board, player)\n",
    "\n",
    "def minimax2(board, depth, alpha, beta, maximizing_player, player):\n",
    "    \"\"\"Thuật toán minimax với alpha-beta pruning.\"\"\"\n",
    "    opponent = 'O' if player == 'X' else 'X'\n",
    "\n",
    "    if depth == 0 or is_terminal(board):\n",
    "        return evaluate1(board, player), None\n",
    "\n",
    "    best_move = None\n",
    "    if maximizing_player:\n",
    "        max_eval = -math.inf\n",
    "        for move in order_moves(board, get_valid_moves(board)):\n",
    "            i, j = move\n",
    "            board[i][j] = player\n",
    "            eval, _ = minimax1(board, depth - 1, alpha, beta, False, player)\n",
    "            board[i][j] = ' '\n",
    "            if eval > max_eval:\n",
    "                max_eval = eval\n",
    "                best_move = move\n",
    "            alpha = max(alpha, eval)\n",
    "            if beta <= alpha:\n",
    "                break\n",
    "        return max_eval, best_move\n",
    "    else:\n",
    "        min_eval = math.inf\n",
    "        for move in order_moves(board, get_valid_moves(board)):\n",
    "            i, j = move\n",
    "            board[i][j] = opponent\n",
    "            eval, _ = minimax1(board, depth - 1, alpha, beta, True, player)\n",
    "            board[i][j] = ' '\n",
    "            if eval < min_eval:\n",
    "                min_eval = eval\n",
    "                best_move = move\n",
    "            beta = min(beta, eval)\n",
    "            if beta <= alpha:\n",
    "                break\n",
    "        return min_eval, best_move"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Heuristic giúp agent đưa ra quyết định tốt hơn ở các trạng thái chưa kết thúc.\n",
    "\n",
    "Có thể mở rộng để đánh giá vị trí ô (trung tâm, góc), hoặc thế “fork”.\n",
    "\n",
    "Kết hợp với Alpha-Beta Pruning, agent sẽ nhanh hơn và thông minh hơn."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cutting off search \n",
    "\n",
    "Modify your Minimax Search with Alpha-Beta Pruning to cut off search at a specified depth and use the heuristic evaluation function. Experiment with different cutoff values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mục tiêu\n",
    "- Sửa đổi thuật toán Minimax để dừng tìm kiếm ở độ sâu xác định (cutoff depth).\n",
    "\n",
    "- Khi đạt đến độ sâu đó, sử dụng hàm đánh giá heuristic thay vì tiếp tục duyệt cây.\n",
    "\n",
    "- Thử nghiệm với các giá trị depth khác nhau để xem ảnh hưởng đến hiệu suất và chất lượng nước đi."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cutoff depth = 2 → Move: (1, 1) | Time: 0.0007 giây\n",
      "Cutoff depth = 4 → Move: (1, 1) | Time: 0.0064 giây\n",
      "Cutoff depth = 6 → Move: (1, 1) | Time: 0.0478 giây\n"
     ]
    }
   ],
   "source": [
    "# Your code/ answer goes here.\n",
    "# Minimax với cutoff và heuristic\n",
    "def minimax_cutoff(board, depth, alpha, beta, maximizing_player, player, cutoff_depth):\n",
    "    opponent = 'O' if player == 'X' else 'X'\n",
    "\n",
    "    if depth == 0 or is_terminal(board):\n",
    "        return heuristic_evaluate(board, player), None\n",
    "\n",
    "    best_move = None\n",
    "    moves = order_moves(board, get_valid_moves(board))\n",
    "\n",
    "    if maximizing_player:\n",
    "        max_eval = -math.inf\n",
    "        for move in moves:\n",
    "            i, j = move\n",
    "            board[i][j] = player\n",
    "            eval, _ = minimax_cutoff(board, depth - 1, alpha, beta, False, player, cutoff_depth)\n",
    "            board[i][j] = ' '\n",
    "            if eval > max_eval:\n",
    "                max_eval = eval\n",
    "                best_move = move\n",
    "            alpha = max(alpha, eval)\n",
    "            if beta <= alpha:\n",
    "                break\n",
    "        return max_eval, best_move\n",
    "    else:\n",
    "        min_eval = math.inf\n",
    "        for move in moves:\n",
    "            i, j = move\n",
    "            board[i][j] = opponent\n",
    "            eval, _ = minimax_cutoff(board, depth - 1, alpha, beta, True, player, cutoff_depth)\n",
    "            board[i][j] = ' '\n",
    "            if eval < min_eval:\n",
    "                min_eval = eval\n",
    "                best_move = move\n",
    "            beta = min(beta, eval)\n",
    "            if beta <= alpha:\n",
    "                break\n",
    "        return min_eval, best_move\n",
    "    \n",
    "# Agent sử dụng minimax với cutoff và heuristic\n",
    "def heuristic_agent(board, player, cutoff_depth=4):\n",
    "    _, move = minimax_cutoff(board, depth=cutoff_depth, alpha=-math.inf, beta=math.inf,\n",
    "                             maximizing_player=True, player=player, cutoff_depth=cutoff_depth)\n",
    "    if move is None:\n",
    "        move = random.choice(get_valid_moves(board))\n",
    "    return move\n",
    "\n",
    "# Thử nghiệm với các giá trị cutoff\n",
    "import time\n",
    "\n",
    "for cutoff in [2, 4, 6]:\n",
    "    board = create_empty_board(3)\n",
    "    start = time.time()\n",
    "    move = heuristic_agent(board, 'X', cutoff_depth=cutoff)\n",
    "    end = time.time()\n",
    "    print(f\"Cutoff depth = {cutoff} → Move: {move} | Time: {end - start:.4f} giây\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many nodes are searched and how long does it take to make a move? Start with a smaller board with 4 columns and make the board larger by adding columns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mục tiêu\n",
    "- Bắt đầu với bàn cờ nhỏ (ví dụ 3x4), sau đó tăng dần số cột (4x4, 4x5, 4x6…).\n",
    "\n",
    "- Đo:\n",
    "\n",
    "    - Số lượng node được duyệt trong Minimax với Alpha-Beta Pruning.\n",
    "\n",
    "    - Thời gian tính toán để chọn nước đi.\n",
    "\n",
    "- Phân tích ảnh hưởng của kích thước bàn cờ đến hiệu suất thuật toán."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Board Size | Nodes Searched | Time (s) |\n",
      "|------------|----------------|----------|\n",
      "| 4x4      | 344           | 0.0037 |\n",
      "| 4x5      | 510           | 0.0137 |\n",
      "| 4x6      | 708           | 0.0125 |\n",
      "| 4x7      | 938           | 0.0234 |\n"
     ]
    }
   ],
   "source": [
    "# Your code/ answer goes here.\n",
    "def count_nodes_minimax_cutoff(board, depth, alpha, beta, maximizing_player, player, node_counter):\n",
    "    \"\"\"Minimax có cutoff và đếm số node duyệt.\"\"\"\n",
    "    node_counter[0] += 1\n",
    "    opponent = 'O' if player == 'X' else 'X'\n",
    "\n",
    "    if depth == 0 or is_terminal(board):\n",
    "        return heuristic_evaluate(board, player), None\n",
    "\n",
    "    best_move = None\n",
    "    moves = order_moves(board, get_valid_moves(board))\n",
    "\n",
    "    if maximizing_player:\n",
    "        max_eval = -math.inf\n",
    "        for move in moves:\n",
    "            i, j = move\n",
    "            board[i][j] = player\n",
    "            eval, _ = count_nodes_minimax_cutoff(board, depth - 1, alpha, beta, False, player, node_counter)\n",
    "            board[i][j] = ' '\n",
    "            if eval > max_eval:\n",
    "                max_eval = eval\n",
    "                best_move = move\n",
    "            alpha = max(alpha, eval)\n",
    "            if beta <= alpha:\n",
    "                break\n",
    "        return max_eval, best_move\n",
    "    else:\n",
    "        min_eval = math.inf\n",
    "        for move in moves:\n",
    "            i, j = move\n",
    "            board[i][j] = opponent\n",
    "            eval, _ = count_nodes_minimax_cutoff(board, depth - 1, alpha, beta, True, player, node_counter)\n",
    "            board[i][j] = ' '\n",
    "            if eval < min_eval:\n",
    "                min_eval = eval\n",
    "                best_move = move\n",
    "            beta = min(beta, eval)\n",
    "            if beta <= alpha:\n",
    "                break\n",
    "        return min_eval, best_move\n",
    "\n",
    "# Thử nghiệm với các kích thước bàn cờ\n",
    "def create_board(rows, cols):\n",
    "    return [[' ' for _ in range(cols)] for _ in range(rows)]\n",
    "\n",
    "print(\"| Board Size | Nodes Searched | Time (s) |\")\n",
    "print(\"|------------|----------------|----------|\")\n",
    "\n",
    "for cols in range(4, 8):  # từ 4 đến 7 cột\n",
    "    board = create_board(4, cols)\n",
    "    node_counter = [0]\n",
    "    start = time.time()\n",
    "    _, move = count_nodes_minimax_cutoff(board, depth=3, alpha=-math.inf, beta=math.inf,\n",
    "                                         maximizing_player=True, player='X', node_counter=node_counter)\n",
    "    end = time.time()\n",
    "    print(f\"| 4x{cols}      | {node_counter[0]}           | {end - start:.4f} |\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Playtime\n",
    "\n",
    "Let two heuristic search agents (different cutoff depth, different heuristic evaluation function) compete against each other on a reasonably sized board. Since there is no randomness, you only need to let them play once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kết quả trận đấu:\n",
      "['O', 'X', 'O', 'O', 'O']\n",
      "['O', 'X', 'X', 'X', 'O']\n",
      "['X', 'X', 'X', 'X', 'O']\n",
      "[' ', 'X', 'X', 'X', 'O']\n",
      "['O', 'O', ' ', ' ', 'O']\n",
      "Người thắng: O\n"
     ]
    }
   ],
   "source": [
    "# Your code/ answer goes here.\n",
    "\n",
    "# 2 hàm heuristic khác nhau\n",
    "def heuristic_v1(board, player):\n",
    "    # Đơn giản: đếm số dòng/cột/chéo chưa bị chặn\n",
    "    opponent = 'O' if player == 'X' else 'X'\n",
    "    score = 0\n",
    "    size = len(board)\n",
    "    lines = []\n",
    "\n",
    "    for i in range(size):\n",
    "        lines.append(board[i])\n",
    "        lines.append([board[j][i] for j in range(size)])\n",
    "\n",
    "    lines.append([board[i][i] for i in range(size)])\n",
    "    lines.append([board[i][size - 1 - i] for i in range(size)])\n",
    "\n",
    "    for line in lines:\n",
    "        if opponent not in line:\n",
    "            score += line.count(player)\n",
    "    return score\n",
    "\n",
    "def heuristic_v2(board, player):\n",
    "    # Nâng cao: thưởng theo số lượng quân liên tiếp\n",
    "    opponent = 'O' if player == 'X' else 'X'\n",
    "    score = 0\n",
    "    size = len(board)\n",
    "    lines = []\n",
    "\n",
    "    for i in range(size):\n",
    "        lines.append(board[i])\n",
    "        lines.append([board[j][i] for j in range(size)])\n",
    "\n",
    "    lines.append([board[i][i] for i in range(size)])\n",
    "    lines.append([board[i][size - 1 - i] for i in range(size)])\n",
    "\n",
    "    for line in lines:\n",
    "        if opponent not in line:\n",
    "            count = line.count(player)\n",
    "            score += 10 ** count\n",
    "        elif player not in line:\n",
    "            count = line.count(opponent)\n",
    "            score -= 10 ** count\n",
    "    return score\n",
    "\n",
    "# Agent sử dụng heuristic và cutoff\n",
    "def custom_agent(board, player, cutoff_depth, heuristic_fn):\n",
    "    def minimax_custom(board, depth, alpha, beta, maximizing_player):\n",
    "        if depth == 0 or is_terminal(board):\n",
    "            return heuristic_fn(board, player), None\n",
    "\n",
    "        best_move = None\n",
    "        moves = order_moves(board, get_valid_moves(board))\n",
    "        opponent = 'O' if player == 'X' else 'X'\n",
    "\n",
    "        if maximizing_player:\n",
    "            max_eval = -math.inf\n",
    "            for move in moves:\n",
    "                i, j = move\n",
    "                board[i][j] = player\n",
    "                eval, _ = minimax_custom(board, depth - 1, alpha, beta, False)\n",
    "                board[i][j] = ' '\n",
    "                if eval > max_eval:\n",
    "                    max_eval = eval\n",
    "                    best_move = move\n",
    "                alpha = max(alpha, eval)\n",
    "                if beta <= alpha:\n",
    "                    break\n",
    "            return max_eval, best_move\n",
    "        else:\n",
    "            min_eval = math.inf\n",
    "            for move in moves:\n",
    "                i, j = move\n",
    "                board[i][j] = opponent\n",
    "                eval, _ = minimax_custom(board, depth - 1, alpha, beta, True)\n",
    "                board[i][j] = ' '\n",
    "                if eval < min_eval:\n",
    "                    min_eval = eval\n",
    "                    best_move = move\n",
    "                beta = min(beta, eval)\n",
    "                if beta <= alpha:\n",
    "                    break\n",
    "            return min_eval, best_move\n",
    "\n",
    "    _, move = minimax_custom(board, cutoff_depth, -math.inf, math.inf, True)\n",
    "    if move is None:\n",
    "        move = random.choice(get_valid_moves(board))\n",
    "    return move\n",
    "\n",
    "# Đấu một ván\n",
    "def play_custom_match(board_size=4):\n",
    "    board = create_empty_board(board_size)\n",
    "    current_player = 'X'\n",
    "    agents = {\n",
    "        'X': lambda b, p: custom_agent(b, p, cutoff_depth=3, heuristic_fn=heuristic_v1),\n",
    "        'O': lambda b, p: custom_agent(b, p, cutoff_depth=5, heuristic_fn=heuristic_v2)\n",
    "    }\n",
    "\n",
    "    while not is_terminal(board):\n",
    "        move = agents[current_player](board, current_player)\n",
    "        i, j = move\n",
    "        board[i][j] = current_player\n",
    "        current_player = 'O' if current_player == 'X' else 'X'\n",
    "\n",
    "    winner = get_winner(board)\n",
    "    print(\"Kết quả trận đấu:\")\n",
    "    for row in board:\n",
    "        print(row)\n",
    "    print(\"Người thắng:\", winner if winner else \"Hòa\")\n",
    "\n",
    "play_custom_match(board_size=5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nhận xét\n",
    "- Agent 'X' dùng heuristic đơn giản và độ sâu thấp hơn.\n",
    "\n",
    "- Agent 'O' dùng heuristic nâng cao và độ sâu lớn hơn.\n",
    "\n",
    "- Kết quả phản ánh sự khác biệt về chiến lược và khả năng phân tích."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
